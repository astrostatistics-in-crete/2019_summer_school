{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering - Session / Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Clustering\" refers to a number of algorithms that can identify structures within a dataset, i.e. concentrations of datapoints or overdensities. \n",
    " \n",
    "Clustering is an **unsupervised learning** technique, meaning prior knowledge of the clusters (e.g. number, properties) is needed. However, it is possible to use a labelled training sample to calibrate the hyperparameters.\n",
    "\n",
    "## 1. Clustering algorithms overview\n",
    "\n",
    "In this session we will present a broad overview of classification algorithms.\n",
    "We will first showcase a set of **sklearn** algorithms, and then describe most of them.\n",
    "\n",
    "The rest are left for the student to study or experiment during the following workshop section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colors\n",
    "import warnings\n",
    "warnings.filterwarnings(action='once')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE TOY DATASET\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "n_samples = 1500\n",
    "noisy_circles = datasets.make_circles(n_samples=n_samples, factor=.5,\n",
    "                                      noise=.05)\n",
    "noisy_moons = datasets.make_moons(n_samples=n_samples, noise=.05)\n",
    "blobs = datasets.make_blobs(n_samples=n_samples, random_state=8)\n",
    "no_structure = np.random.rand(n_samples, 2), None\n",
    "\n",
    "# Anisotropicly distributed data\n",
    "random_state = 170\n",
    "X, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)\n",
    "transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
    "X_aniso = np.dot(X, transformation)\n",
    "aniso = (X_aniso, y)\n",
    "\n",
    "# blobs with varied variances\n",
    "varied = datasets.make_blobs(n_samples=n_samples,\n",
    "                             cluster_std=[1.0, 2.5, 0.5],\n",
    "                             random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETTING UP CLUSTERING PARAMETERS\n",
    "\n",
    "default_base = {'quantile': .3,\n",
    "                'eps': .3,\n",
    "                'damping': .9,\n",
    "                'preference': -200,\n",
    "                'n_neighbors': 10,\n",
    "                'n_clusters': 3}\n",
    "\n",
    "datasets = [\n",
    "    (noisy_circles, {'damping': .77, 'preference': -240,\n",
    "                     'quantile': .2, 'n_clusters': 2}),\n",
    "    (noisy_moons, {'damping': .75, 'preference': -220, 'n_clusters': 2}),\n",
    "    (varied, {'eps': .18, 'n_neighbors': 2}),\n",
    "    (aniso, {'eps': .15, 'n_neighbors': 2}),\n",
    "    (blobs, {}),\n",
    "    (no_structure, {})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUNNIG CLUSTERING ALGORITHMS\n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn import cluster, mixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import cycle, islice\n",
    "\n",
    "plt.figure(figsize=(9 * 2 + 3, 12.5))\n",
    "plt.subplots_adjust(left=.02, right=.98, bottom=.001, top=.96, wspace=.05,\n",
    "                    hspace=.01)\n",
    "\n",
    "plot_num = 1\n",
    "\n",
    "\n",
    "for i_dataset, (dataset, algo_params) in enumerate(datasets):\n",
    "    # update parameters with dataset-specific values\n",
    "    params = default_base.copy()\n",
    "    params.update(algo_params)\n",
    "\n",
    "    X, y = dataset\n",
    "\n",
    "    # normalize dataset for easier parameter selection\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    # estimate bandwidth for mean shift\n",
    "    bandwidth = cluster.estimate_bandwidth(X, quantile=params['quantile'])\n",
    "\n",
    "    # connectivity matrix for structured Ward\n",
    "    connectivity = kneighbors_graph(\n",
    "        X, n_neighbors=params['n_neighbors'], include_self=False)\n",
    "    # make connectivity symmetric\n",
    "    connectivity = 0.5 * (connectivity + connectivity.T)\n",
    "\n",
    "    # ============\n",
    "    # Create cluster objects\n",
    "    # ============\n",
    "    ms = cluster.MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
    "    two_means = cluster.MiniBatchKMeans(n_clusters=params['n_clusters'])\n",
    "    ward = cluster.AgglomerativeClustering(\n",
    "        n_clusters=params['n_clusters'], linkage='ward',\n",
    "        connectivity=connectivity)\n",
    "    spectral = cluster.SpectralClustering(\n",
    "        n_clusters=params['n_clusters'], eigen_solver='arpack',\n",
    "        affinity=\"nearest_neighbors\")\n",
    "    dbscan = cluster.DBSCAN(eps=params['eps'])\n",
    "    affinity_propagation = cluster.AffinityPropagation(\n",
    "        damping=params['damping'], preference=params['preference'])\n",
    "    average_linkage = cluster.AgglomerativeClustering(\n",
    "        linkage=\"average\", affinity=\"cityblock\",\n",
    "        n_clusters=params['n_clusters'], connectivity=connectivity)\n",
    "    birch = cluster.Birch(n_clusters=params['n_clusters'])\n",
    "    gmm = mixture.GaussianMixture(\n",
    "        n_components=params['n_clusters'], covariance_type='full')\n",
    "\n",
    "    clustering_algorithms = (\n",
    "        ('MiniBatchKMeans', two_means),\n",
    "        ('AffinityPropagation', affinity_propagation),\n",
    "        ('MeanShift', ms),\n",
    "        ('SpectralClustering', spectral),\n",
    "        ('Ward', ward),\n",
    "        ('AgglomerativeClustering', average_linkage),\n",
    "        ('DBSCAN', dbscan),\n",
    "        ('Birch', birch),\n",
    "        ('GaussianMixture', gmm)\n",
    "    )\n",
    "\n",
    "    for name, algorithm in clustering_algorithms:\n",
    "        t0 = time.time()\n",
    "\n",
    "        # catch warnings related to kneighbors_graph\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\n",
    "                \"ignore\",\n",
    "                message=\"the number of connected components of the \" +\n",
    "                \"connectivity matrix is [0-9]{1,2}\" +\n",
    "                \" > 1. Completing it to avoid stopping the tree early.\",\n",
    "                category=UserWarning)\n",
    "            warnings.filterwarnings(\n",
    "                \"ignore\",\n",
    "                message=\"Graph is not fully connected, spectral embedding\" +\n",
    "                \" may not work as expected.\",\n",
    "                category=UserWarning)\n",
    "            algorithm.fit(X)\n",
    "\n",
    "        t1 = time.time()\n",
    "        if hasattr(algorithm, 'labels_'):\n",
    "            y_pred = algorithm.labels_.astype(np.int)\n",
    "        else:\n",
    "            y_pred = algorithm.predict(X)\n",
    "\n",
    "        plt.subplot(len(datasets), len(clustering_algorithms), plot_num)\n",
    "        if i_dataset == 0:\n",
    "            plt.title(name, size=18)\n",
    "\n",
    "        colors = np.array(list(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a',\n",
    "                                             '#f781bf', '#a65628', '#984ea3',\n",
    "                                             '#999999', '#e41a1c', '#dede00']),\n",
    "                                      int(max(y_pred) + 1))))\n",
    "        plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[y_pred])\n",
    "\n",
    "        plt.xlim(-2.5, 2.5)\n",
    "        plt.ylim(-2.5, 2.5)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "        plt.text(.99, .01, ('%.2fs' % (t1 - t0)).lstrip('0'),\n",
    "                 transform=plt.gca().transAxes, size=15,\n",
    "                 horizontalalignment='right')\n",
    "        plot_num += 1\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The showcase above is reproduced from **sklearn**. For more info, see:\n",
    "http://scikit-learn.org/stable/modules/clustering.html\n",
    "\n",
    "The last dataset is an example of a 'null' situation for clustering: the data is homogeneous, and there is no good clustering. \n",
    "\n",
    "Important take-home points:\n",
    "\n",
    "> * Not all the algorithms identify the same number of clusters\n",
    "> * Some algorithms are more sensitive to parameter values than others\n",
    "> * Some algorithms cannot guess the number of clusters, but only find them\n",
    "> * Some algorithms can be faster\n",
    "> * Same algorithms are not computationally efficient with large samples\n",
    "> * The intuitive clustering might not apply to very high dimensional data\n",
    "\n",
    "### How do they separate the clusters?\n",
    "\n",
    "Tipically, clustering algorithms use 2 types of hyperparamters:\n",
    "\n",
    "> * The properties of the data themselves\n",
    "    <br>\n",
    "    _e.g. density of points (minimum number of points and the area), distance between points, etc._\n",
    "> * The number of clusters\n",
    "    <br>\n",
    "    _might be provided by the user or dinamically derived_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Algorithm: K-means\n",
    "\n",
    "The K-means algorithm tries to partition a sample of N observations (with each observation being a $d$-dimensional vector) into $k$ individual clusters $C_k$. The goal is to minimize the within-cluster sum-of-squares (or **inertia**) of the observations:\n",
    "\n",
    "$$min \\left (  \\sum_{k=1}^{K} \\sum_{i \\epsilon C_k} ||x_i-\\mu_k||^2 \\right )$$\n",
    "\n",
    "where $\\mu_k=\\frac{1}{N_k}\\sum_{i \\epsilon C_k} x_i$ is the mean/centroid of the $N_k$ points included in each of the $C_k$ clusters. \n",
    "\n",
    "<table><tr>\n",
    "    <td width=400>\n",
    "        <img src=\"images/kmeans.gif\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 2. Evolution of K-means centroids through iterations.\n",
    "            <br>\n",
    "            (From [here](https://dashee87.github.io/data%20science/general/Clustering-with-Scikit-with-GIFs/))\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "**Steps**:\n",
    "1. Initiate algorithm by selecting $k$ means\n",
    "<br>\n",
    "*e.g. select randomly $k$ observations as initial means - see also [Wiki:K-means initialization](http://en.wikipedia.org/wiki/K-means_clustering#Initialization_methods)*\n",
    "2. Assign each observation to the nearest cluster\n",
    "3. Calculate the new mean value for each cluster $C_k$ according to the new observations assiged\n",
    "4. Repeat steps 2 and 3 up to the point that there are no updates in the assigments to the clusters.\n",
    "\n",
    "A globally optimal minimum is not guaranteed (might converge to a local minimum). This is highly dependent on the initialization of the centroids. This is why, in practice, K-means is run multiple times with different starting values selecting the result with the lowest sum-of-squares error. To improve on that we can initially select centrodis that are generally distant from each other. See more [here](https://www.jeremyjordan.me/grouping-data-points-with-k-means-clustering/).\n",
    "\n",
    "#### Complexity\n",
    "\n",
    "$O(knT)$, where k, n and T are the number of clusters, samples and iterations, respectively.\n",
    "\n",
    "### Pros\n",
    "\n",
    "- Simple and intuitive\n",
    "\n",
    "### Cons\n",
    "\n",
    "- The number of clusters (K) must be provided (or cross-validated)\n",
    "- There is an inherit assumption of isotropic clusters\n",
    "- Inertia is not a normalized metric: lower values are better , but as the dimensions increase so does the inertia\n",
    "\n",
    "### 2.1 Mini Batch K-means\n",
    "\n",
    "For faster computations the sklearn offers the [Mini Batch K-means](http://scikit-learn.org/stable/modules/clustering.html#mini-batch-kmeans) method which simply breaks the initial set of observations/data points to smaller randomly selected subsamples.\n",
    "\n",
    "For each subsample in the mini batch the assigned centroid is updated by taking into account the average of that subsample and all previous subsamples assigned to that centroid. This is prepeated until the predefined number of iterations is reached. Its results are generally only slightly worset then the standard algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Algorithm: Mean Shift\n",
    "\n",
    "Mean Shift identifies arbitrarily shaped clusters (blobs) by locating the peaks of a density estimate of the data.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "The algorithm iteratively shifts the centroid of a cluster \"climbing\" the peak of the density distribution of the nearby data points: this is effectively a \"gradient search\". But how to define the \"shape\" of the gradient? The assumed distribution is given by a local convolution between the data and a kernel function, e.g. Gaussian:\n",
    "\n",
    "> $K(x_i-x)=e^{-c||x_i-x||^2}$) \n",
    "\n",
    "where $x_i$ is sample $i$ and $x$ is the cluster centroid.\n",
    "Apart from the kernel function, the user must also define a neighborhood $N(x)$ over which to estimate the density (also called **bandwith** $h$).\n",
    "This implicitly sets the number of K of clusters which will be found.\n",
    "\n",
    "In practice, at each iteration the algorithm calculates a **mean shift** vector which tells where to re-place the cluster center:\n",
    "\n",
    "> $m(x)-x$\n",
    "\n",
    "where $m(x)$ is the new expected centroid, calculated as:\n",
    "\n",
    "$$m(x)=\\frac{ \\sum_{x_i\\epsilon N(x)}K(x_i-x)x_i}{\\sum_{x_i\\epsilon N(x)}K(x_i-x)}$$.\n",
    "\n",
    "The search stops when the update in the centroid is below some threshold.\n",
    "\n",
    "<table><tr>\n",
    "    <td width=300>\n",
    "        <img src=\"images/mean_shift.gif\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 3. Evolution of a Mean Shift centroid through iterations.\n",
    "            The centroid \"climbs\" towards the center of the density distribution.\n",
    "            <br>\n",
    "            (From [here](https://dashee87.github.io/data%20science/general/Clustering-with-Scikit-with-GIFs/))\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "#### Complexity\n",
    "\n",
    "$O(Tn^{2})$, where n and T are the number of samples and iterations, respectively.\n",
    "\n",
    "### Pros\n",
    "\n",
    "- Guaranteed to converge\n",
    "\n",
    "### Cons\n",
    "\n",
    "- The number of clusters implicitly set\n",
    "- There is an inherit assumption of isotropic clusters\n",
    "- Computationally expensive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hierarchical Clustering\n",
    "\n",
    "This type of clustering groups points by ranking them **bottom-up** (agglomerative) or **top-down** (divisive). It merges close points into clusters based on the distance between them.\n",
    "\n",
    "### 4.1. Agglomerative Clustering\n",
    "\n",
    "This is an example of the **bottom-up** case.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Initiate with K$_{clusters}$ = N$_{points}$\n",
    "2. Calculate the distances between all the pairs of points\n",
    "3. Iteratively connect the nearest pairs of points until obtaining a single cluster\n",
    "<br>\n",
    "_(see the dendrogram below)_\n",
    "4. Merge all clusters below a threshold, which could be:\n",
    "    - a given number of clusters (stop at K clusters)\n",
    "    - a given separation\n",
    "\n",
    "<table><tr>\n",
    "    <td width=350>\n",
    "        <img src=\"images/dendogram.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 4.1.a. Construction of dendogram and application of decision threshold.\n",
    "        </center>\n",
    "    </td>\n",
    "    <td width=400>\n",
    "        <img src=\"images/hierarchical.gif\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 4.1.b.Construction of dendogram for Hierarchical Clustering.\n",
    "            <br>\n",
    "            (From [here](https://dashee87.github.io/data%20science/general/Clustering-with-Scikit-with-GIFs/))\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "#### Type of  point linking for Agglomerative Clustering\n",
    "\n",
    "For additional complexity, one can chose other solutions other than the pair-wise coupling of points.\n",
    "\n",
    "<tr><table>\n",
    "    <td width=400>\n",
    "        <img src=\"images/cluster_distances.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 4.2.Type of point linking for Agglomerative Clustering.\n",
    "            <br>\n",
    "            (From [here](https://www.youtube.com/watch?v=VMyXc3SiEqs))\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "#### Complexity\n",
    "\n",
    "$O(n^2 d + n^3)$, where n and T are the number of samples and iterations, respectively.\n",
    "\n",
    "very slow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Algorithm: DBSCAN\n",
    "**(Density Based Spatial Clustering of Applications with Noise)**\n",
    "\n",
    "The DBSCAN algorithm views clusters as areas of high density separated by areas of low density.\n",
    "Thus, clusters found by DBSCAN can be any shape - as opposed to other algorithms (K-means for example) which assume that clusters are convex shaped.\n",
    "\n",
    "It uses 2 parameters:\n",
    "* $eps$ : neighborhood size\n",
    "* $minPts$ : minimum number of points for a neighborhood to be considered dense\n",
    "\n",
    "<table><tr>\n",
    "    <td width=350>\n",
    "        <img src=\"images/dbscan.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 5.1.a. Definition of **core**, **border**, and **noise** points, according to DBSCAN.\n",
    "        </center>\n",
    "    </td>\n",
    "    <td width=400>\n",
    "        <img src=\"images/dbscan-smiley.gif\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 5.1.b.DBSCAN constructing clusters.\n",
    "            <br>\n",
    "            (From [here](http://arogozhnikov.github.io/images/opera/post/clustering-dbscan-smiley.gif))\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "With 1 scan we can label the points as: **core**, **border**, **noise**. How?\n",
    "\n",
    "-  A point $p$ is defined **core** if at least $minPts$ points are within the area defined by $eps$\n",
    "-  A **border** point is a non-core point that has at least 1 core point in its neighborhood\n",
    "-  A **noise** point is neither a core nor a border point. There represent outliers in the data set\n",
    "\n",
    "Defined these, DBSCAN operates as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps:\n",
    "\n",
    "1. Algorithm picks 1 unassigned core point ($p_c$)\n",
    "2. Let $p_c$ be the current point being explored\n",
    "3. Add all points ($q$) of $p_c$’s neighborhood to the same cluster\n",
    "4. Some of these $q$ points are also core points so: recursively apply this search on each unexplored core point of this neighborhood\n",
    "5. When all neighbourhood points have been joined, DBSCAN proceeds with a new cluster\n",
    "\n",
    "Process ends when all core points have been assigned to a cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pros\n",
    "- Any number of clusters\n",
    "- Clusters of varying size and shape\n",
    "- Finds and ignores outliers\n",
    "\n",
    "### Cons\n",
    "- Relatively slow\n",
    "- Extremely sensitive to parameters choice\n",
    "- In rare cases, border points move to an other cluster when DBSCAN is re-run\n",
    "- Serious troubles with clusters with varying density\n",
    "  <br>\n",
    "  _(OPTICS and HDBSCAN are variations which address this problem)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary\n",
    "| Algorithm                | fixed n$_{clusters}$ ? |\n",
    "| :----------------------: | :--: |\n",
    "| KMeans                   | yes  |\n",
    "| Mean Shift               | implicitly |\n",
    "| Agglomerative Clustering | yes or no |\n",
    "| DBSCAN                   | no |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. EXERCISE: Classify Star-forming objects in a BPT diagram\n",
    "\n",
    "The \"Baldwin, Phillips & Terlevich\" (BPT) diagrams are used to distinguish sources based on specific spectral emission lines. The strengths of these lines depend on the heating source.\n",
    "See e.g.: https://ned.ipac.caltech.edu/level5/Glossary/Essay_bpt.html\n",
    "\n",
    "BPTs allow to distinguish:\n",
    "> - AGNs (Seyfert)\n",
    "> - LINERs\n",
    "> - Star-forming galaxies\n",
    "> - Composite objects\n",
    "\n",
    "<table><tr>\n",
    "    <td width=600>\n",
    "        <img src=\"images/BPT.png\" width=600>\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure A. Example of classification via BPT diagram.\n",
    "            Theoretical or observationally-calibrated curves allow to distinguish\n",
    "            the different sub-populations.\n",
    "            <br>\n",
    "            (From [Parra et al. (2010), ApJ, 720, 555](https://ui.adsabs.harvard.edu/abs/2010ApJ...720..555P/abstract))\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "\n",
    "### The sample\n",
    "\n",
    "We will use the data by [Stampoulis et al. (2019), MNRAS, 485, 1085](https://ui.adsabs.harvard.edu/abs/2019MNRAS.485.1085S/abstract), which provides the OIII, NII, SII, and OI diagnostics for ~130 000 objects.\n",
    "\n",
    "The work also gives classifications, which we will use as a reference.\n",
    "\n",
    "### TASK A.1: Find the best clustering algorithm for separating star-forming objects\n",
    "\n",
    "### TASK A.2: Plot results and check consistency with BPT theoretical curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and setting up the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING DATA STRUCTURE\n",
    "\n",
    "# > Loading the emission line data and classifications from Stampoulis+19:\n",
    "\n",
    "PATH_Stampoulis_data = \"data/Stampoulis+19_Table_2.csv\"\n",
    "\n",
    "data = np.genfromtxt(PATH_Stampoulis_data, delimiter=\",\")\n",
    "# The data file is organized in 138799 lines (i.e. different objects), and 12 columns\n",
    "\n",
    "# To check file dimensions:\n",
    "# print(data.shape)\n",
    "\n",
    "ID               = data[:,0]  # object ID\n",
    "NII_diagnostic   = data[:,3]  # log10 ( NII_6584  / H_alpha )\n",
    "SII_diagnostic   = data[:,4]  # log10 ( SII_6717  / H_alpha )\n",
    "OI_diagnostic    = data[:,5]  # log10 ( OI        / H_alpha )\n",
    "OIII_diagnostic  = data[:,6]  # log10 ( OIII_5007 / H_beta )\n",
    "\n",
    "labels = np.genfromtxt(PATH_Stampoulis_data, delimiter=',', usecols=-1, dtype=str)\n",
    "# reading labels from last column\n",
    "# Activity class labelling scheme:\n",
    "#   0 <-> SFG (Star Forming Galaxy)\n",
    "#   1 <-> SEY (Seyfert)\n",
    "#   2 <-> LIN (LINER)\n",
    "#   3 <-> COM (Composite)\n",
    "\n",
    "# Dictionary containg class name and associated label:\n",
    "from collections import OrderedDict\n",
    "classes = OrderedDict()\n",
    "classes[\"SFG\"] = 0\n",
    "classes[\"SEY\"] = 1\n",
    "classes[\"LIN\"] = 2\n",
    "classes[\"COM\"] = 3\n",
    "\n",
    "labels = [int(float(label)) for label in labels]\n",
    "# converting labels from strings to integers\n",
    "\n",
    "# > Organizing data in an analysis-ready fashion:\n",
    "X_sample = np.stack((OIII_diagnostic,NII_diagnostic,SII_diagnostic,OI_diagnostic),axis=-1)\n",
    "y_sample = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will Use only 1 every <sampling_factor> objects for two reasons:\n",
    "\n",
    "> - to speed up the exercise\n",
    "> - to avoid crashes due to memory limitations.\n",
    "\n",
    "You can try to use the full sample when confident with the setup (and your computer power!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUGGESTION: Use only 1 every <sampling_factor> objects \n",
    "\n",
    "sampling_factor = 50\n",
    "# sample 1 every <sampling_factor> data to avoid computational delay\n",
    "\n",
    "X = X_sample[::sampling_factor]\n",
    "y = y_sample[::sampling_factor]\n",
    "\n",
    "print('Sample shape:')\n",
    "print(\"_____________________________________\")\n",
    "print('  X  | ' + str(X.shape))\n",
    "print('     | ' + str(X.shape[0]) + ' sammples x ' + str(X.shape[1]) + ' diagnostics' )\n",
    "print(\"-----|-------------------------------\")\n",
    "print('  y  | ' + str(len(y)) + ' labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although available, we will not use the labels for the analysis, but only for the first representation of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the data\n",
    "Essentially reproducing Figure 5 in Stampoulis+19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colors\n",
    "\n",
    "# Limit scatter plots (not histograms) in showing a maximum of <N_plot> objects:\n",
    "# for full sample size, use: N_plot = len(X)\n",
    "N_plot = 5000\n",
    "# NOTE: reducing the sample in the plot helps visualizing the density\n",
    "\n",
    "# Creating a colormap where:\n",
    "#   red    <-> SFG\n",
    "#   yellow <-> SEY\n",
    "#   blue   <-> LIN\n",
    "#   green  <-> COM\n",
    "cmap = mpl.colors.ListedColormap(['red','orange','blue','green'])\n",
    "\n",
    "# Remeber that the sample X is organized as:\n",
    "#  X[:,0] <-> OIII_diagnostic\n",
    "#  X[:,1] <-> NII_diagnostic\n",
    "#  X[:,2] <-> SII_diagnostic\n",
    "#  X[:,3] <-> OI_diagnostic\n",
    "\n",
    "\n",
    "# PLOT THE DIAGNOSITCS\n",
    "\n",
    "# > Classification lines\n",
    "#   NII:\n",
    "x1 = np.linspace(-2, 0.05, 100)\n",
    "x2 = np.linspace(-2, 0.47, 100)\n",
    "x3 = np.linspace(-0.1839, 1)\n",
    "ke01_NII = 0.61 / (x1-0.05) + 1.3   # Kewley+01\n",
    "ka03_NII  = 0.61 / (x2-0.47) + 1.19 # Kuffmann+03\n",
    "sc07_NII  = 1.05 * x3 + 0.45        # Schawinski+07\n",
    "#   SII:\n",
    "x4 = np.linspace(-2, 0.05, 100)\n",
    "x5 = np.linspace(-0.3, 1)\n",
    "ke01_SII  = 0.72 / (x4-0.32) + 1.3  # Kewley+01\n",
    "ke06_SII  = 1.89 * x5 + 0.76        # Kewley+06\n",
    "#   OI:\n",
    "x6 = np.linspace(-2, -0.8, 100)\n",
    "x7 = np.linspace(-1.1, 0)\n",
    "ke01_OI = 0.72 / (x6+0.59) + 1.33  # Kewley+01\n",
    "ke06_OI = 1.18 * x7 + 1.30         # Kewley+06\n",
    "\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "fig.subplots_adjust(bottom=0.15, top=0.95, hspace=0.0, left=0.1, right=0.95, wspace=0.4)\n",
    "\n",
    "ylim = [-1.2,1.5] # OIII_diagnostic range\n",
    "\n",
    "# > left plot\n",
    "\n",
    "xlim = [-2,1] # NII_diagnostic range\n",
    "\n",
    "ax = fig.add_subplot(131)\n",
    "im = ax.scatter(X[:, 1], X[:, 0], c=y, s=2, lw=0, cmap=cmap, zorder=2)\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.5))\n",
    "ax.set_xlabel('log([NII]/H$_{α})$', fontsize=14)\n",
    "ax.set_ylabel('log([ΟII]/H$_{β})$', fontsize=14)\n",
    "#\n",
    "ax.plot(x1, ka03_NII, \"--\", color='red',  linewidth = 1.0, label='Ka03')\n",
    "ax.plot(x2, ke01_NII, \"-\",  color='red',  linewidth = 1.0, label='Ke01')\n",
    "ax.plot(x3, sc07_NII, \"-\",  color='blue', linewidth = 1.0, label='Sc07')\n",
    "\n",
    "# legend:\n",
    "ax.text(0.1,0.25, \"SFG\", color='red',    transform=ax.transAxes, fontsize=14)\n",
    "ax.text(0.1,0.20, \"SEY\", color='orange', transform=ax.transAxes, fontsize=14)\n",
    "ax.text(0.1,0.15, \"LIN\", color='blue',   transform=ax.transAxes, fontsize=14)\n",
    "ax.text(0.1,0.10, \"COM\", color='green',  transform=ax.transAxes, fontsize=14)\n",
    "\n",
    "# > central plot\n",
    "\n",
    "xlim = [-1.4,0.7] # SII_diagnostic range\n",
    "\n",
    "ax = fig.add_subplot(132)\n",
    "im = ax.scatter(X[-N_plot:, 2], X[-N_plot:, 0], c=y[-N_plot:], s=2, lw=0, cmap=cmap, zorder=2)\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.5))\n",
    "ax.set_xlabel('log([SII]/H$_{α})$', fontsize=14)\n",
    "ax.set_ylabel('log([ΟII]/H$_{β})$', fontsize=14)\n",
    "#\n",
    "ax.plot(x4, ke01_SII, \"-\",  color='red',  linewidth = 1.0, label='Ke01')\n",
    "ax.plot(x5, ke06_SII, \"-\",  color='blue', linewidth = 1.0, label='Ke06')\n",
    "\n",
    "\n",
    "# > right plot\n",
    "\n",
    "xlim = [-2.2,0.0] # OI_diagnostic range\n",
    "\n",
    "ax = fig.add_subplot(133)\n",
    "im = ax.scatter(X[-N_plot:, 3], X[-N_plot:, 0], c=y[-N_plot:], s=2, lw=0, cmap=cmap, zorder=2)\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.5))\n",
    "ax.set_xlabel('log([OI]/H$_{α})$', fontsize=14)\n",
    "ax.set_ylabel('log([ΟII]/H$_{β})$', fontsize=14)\n",
    "#\n",
    "ax.plot(x6, ke01_OI, \"-\",  color='red',  linewidth = 1.0, label='Ke01')\n",
    "ax.plot(x7, ke06_OI, \"-\",  color='blue', linewidth = 1.0, label='Ke06')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# NOTE: Ignore the warning, due to the plotting o the lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RESPONSE TO A.1 and A.2: Running clustering algorithms and plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn import cluster, mixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import cycle, islice\n",
    "\n",
    "plt.figure(figsize=(9 * 2 + 3, 8))\n",
    "plt.subplots_adjust(left=.02, right=.98, bottom=.001, top=.96, wspace=.05,\n",
    "                    hspace=.01)\n",
    "\n",
    "plot_num = 1\n",
    "\n",
    "params = {'quantile': .08,\n",
    "            'eps': .4,\n",
    "            'damping': .9,\n",
    "            'preference': -600,\n",
    "            'n_neighbors': 10,\n",
    "            'n_clusters': 2}\n",
    "\n",
    "# normalize dataset for easier parameter selection\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# estimate bandwidth for mean shift\n",
    "bandwidth = cluster.estimate_bandwidth(X, quantile=params['quantile'])\n",
    "\n",
    "# connectivity matrix for structured Ward\n",
    "connectivity = kneighbors_graph(\n",
    "    X, n_neighbors=params['n_neighbors'], include_self=False)\n",
    "# make connectivity symmetric\n",
    "connectivity = 0.5 * (connectivity + connectivity.T)\n",
    "\n",
    "# ============\n",
    "# Create cluster objects\n",
    "# ============\n",
    "ms = cluster.MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
    "k_means = cluster.MiniBatchKMeans(n_clusters=params['n_clusters'])\n",
    "ward = cluster.AgglomerativeClustering(\n",
    "    n_clusters=params['n_clusters'], linkage='ward',\n",
    "    connectivity=connectivity)\n",
    "average_linkage = cluster.AgglomerativeClustering(\n",
    "    linkage=\"average\", affinity=\"cityblock\",\n",
    "    n_clusters=params['n_clusters'], connectivity=connectivity)\n",
    "complete_linkage = cluster.AgglomerativeClustering(\n",
    "    linkage=\"complete\", affinity=\"cityblock\",\n",
    "    n_clusters=params['n_clusters'], connectivity=connectivity)\n",
    "spectral = cluster.SpectralClustering(\n",
    "    n_clusters=params['n_clusters'], eigen_solver='arpack',\n",
    "    affinity=\"nearest_neighbors\")\n",
    "dbscan = cluster.DBSCAN(eps=params['eps'])\n",
    "affinity_propagation = cluster.AffinityPropagation(\n",
    "    damping=params['damping'], preference=params['preference'])\n",
    "birch = cluster.Birch(n_clusters=params['n_clusters'])\n",
    "gmm = mixture.GaussianMixture(\n",
    "    n_components=params['n_clusters'], covariance_type='full')\n",
    "\n",
    "clustering_algorithms = (\n",
    "    ('MiniBatchKMeans', k_means),\n",
    "    ('AffinityPropagation', affinity_propagation),\n",
    "    ('MeanShift', ms),\n",
    "    ('SpectralClustering', spectral),\n",
    "    ('Ward', ward),\n",
    "    ('AgglomerativeClustering', average_linkage),\n",
    "    ('DBSCAN', dbscan),\n",
    "    ('Birch', birch),\n",
    "    ('GaussianMixture', gmm)\n",
    ")\n",
    "\n",
    "# restoring normalization for plotting\n",
    "X_plot = X_sample[::sampling_factor]\n",
    "\n",
    "for name, algorithm in clustering_algorithms:\n",
    "    t0 = time.time()\n",
    "\n",
    "    # catch warnings related to kneighbors_graph\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\n",
    "            \"ignore\",\n",
    "            message=\"the number of connected components of the \" +\n",
    "            \"connectivity matrix is [0-9]{1,2}\" +\n",
    "            \" > 1. Completing it to avoid stopping the tree early.\",\n",
    "            category=UserWarning)\n",
    "        warnings.filterwarnings(\n",
    "            \"ignore\",\n",
    "            message=\"Graph is not fully connected, spectral embedding\" +\n",
    "            \" may not work as expected.\",\n",
    "            category=UserWarning)\n",
    "        algorithm.fit(X)\n",
    "\n",
    "    t1 = time.time()\n",
    "    if hasattr(algorithm, 'labels_'):\n",
    "        y_pred = algorithm.labels_.astype(np.int)\n",
    "    else:\n",
    "        y_pred = algorithm.predict(X)\n",
    "        \n",
    "    plt.subplot(1, len(clustering_algorithms), plot_num)\n",
    "    plt.title(name, size=18)\n",
    "\n",
    "    plt.plot(x1, ka03_NII, \"--\", color='grey',  linewidth = 1.0, label='Ka03')\n",
    "    plt.plot(x2, ke01_NII, \"-\",  color='grey',  linewidth = 1.0, label='Ke01')\n",
    "    plt.plot(x3, sc07_NII, \"-\",  color='grey', linewidth = 1.0, label='Sc07')\n",
    "    \n",
    "    colors = np.array(list(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a',\n",
    "                                         '#f781bf', '#a65628', '#984ea3',\n",
    "                                         '#999999', '#e41a1c', '#dede00']),\n",
    "                                  int(max(y_pred) + 1))))\n",
    "    plt.scatter(X_plot[:, 2], X_plot[:, 0], s=2, color=colors[y_pred])\n",
    "    plt.scatter(X_plot[y_pred==-1,2], X_plot[y_pred==-1,0], s=100, facecolors='none', edgecolors='black', color='black', label='background', alpha=0.1, zorder=0)\n",
    "    # marking background class for DBSCAN\n",
    "\n",
    "    plt.gca().set_xlim(-2,1)     # NII_diagnostic range\n",
    "    plt.gca().set_ylim(-1.2,1.5) # OIII_diagnostic range\n",
    "\n",
    "    plt.xticks(np.arange(xlim[0], xlim[1], step=1))\n",
    "    plt.yticks(np.arange(ylim[0], ylim[1], step=0.5))\n",
    "\n",
    "    plt.text(.99, .01, ('%.2fs' % (t1 - t0)).lstrip('0'),\n",
    "             transform=plt.gca().transAxes, size=15,\n",
    "             horizontalalignment='right')\n",
    "    plot_num += 1\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: Which algorithm performs best?\n",
    "[+] click to discover\n",
    "\n",
    "[//]: # \"\n",
    "The Ward algorithm seems to be the best in separating Star-Forming Galaxies.\n",
    "\n",
    "Other algorithms (e.g. Affinity Propagation) might be considered valid if we join some clusters.\n",
    "\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. EXERCISE: Count stellar clusters\n",
    "\n",
    "In [Bitsakis, Bonfini et al. (2017), ApJ, 845, 56]( https://ui.adsabs.harvard.edu/abs/2017ApJ...845...56B/abstract) we describe a technique for the detection of star clusters in the Large Magellanic Cloud. The basic step for the cluster detection was converting the observed images into pixel-maps, where each star was represented by a single pixel.\n",
    "\n",
    "In this exercise we use clustering algorithms to detect clusters in simulated stellar fields, which we already to pixel-maps. Each pixel-map includes an arbitrary (unknown to the student) number of clusters. The first 3 images present increasing noise levels. One last map has been smoothed to simulate observational data.\n",
    "\n",
    "### TASK B.1: Guess the number of stellar clusters in the provided images using cluster algorithms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and visualizing the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# FITS manipulation:\n",
    "from astropy.io import fits\n",
    "\n",
    "PATH_cluster_files = [\n",
    "    \"data/clusters_0.fits\",\n",
    "    \"data/clusters_1.fits\",\n",
    "    \"data/clusters_2.fits\",\n",
    "    \"data/clusters_2_smooth.fits\"]\n",
    "\n",
    "# > Loading fits files:\n",
    "image_data = []\n",
    "\n",
    "for i, PATH_cluster_file in enumerate(PATH_cluster_files):\n",
    "\n",
    "    hdulist = fits.open(PATH_cluster_file)\n",
    "    image_data.append(hdulist[0].data)\n",
    "    hdulist.close()\n",
    "\n",
    "# > Displaying fits files:    \n",
    "fig = plt.figure(figsize=(18, 10))\n",
    "fig.subplots_adjust(bottom=0.15, top=0.95, hspace=0.0, left=0.1, right=0.95, wspace=0.4)\n",
    "\n",
    "for i in range(len(PATH_cluster_files)):\n",
    "\n",
    "    ax = plt.subplot(1, 4, int(i+1))  \n",
    "    ax.imshow(image_data[i], cmap='gray', vmin=-1, vmax=0.5*np.max(image_data[i]), origin='lower')\n",
    "\n",
    "print('All black pixels have a value of %s' % int(np.min(image_data[0])))\n",
    "print('All white pixels have a value of %s' % int(np.max(image_data[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONVERTING FROM IMAGE FORMAT TO STANDARD FORMAT FOR CLASSIFIER\n",
    "\n",
    "'''\n",
    "The image shape loaded with fits.open() is:\n",
    "    <n_pixels, n_pixels>\n",
    "where each element represents an image row (intensity_1, intensity_2, ... , intensity_n).\n",
    "We will convert this to the more convenient shape:\n",
    "    X = <n_pixels^2, 3>\n",
    "where each element represents a single pixel (x, y, intensity).\n",
    "This is the format used by clustering algorithms.\n",
    "'''\n",
    "\n",
    "datasets = []\n",
    "# list containing all images\n",
    "\n",
    "for i, PATH_cluster_file in enumerate(PATH_cluster_files):\n",
    "\n",
    "    x = np.arange(image_data[i].shape[1])\n",
    "    y = np.arange(image_data[i].shape[0])\n",
    "    # NOTE: python inverts i with j index\n",
    "    xx, yy = np.meshgrid(x, y)\n",
    "\n",
    "    X_image = np.array([xx.ravel(), yy.ravel(), image_data[i].ravel()]).T\n",
    "\n",
    "    X_image = X_image[X_image[:,2]!=0]\n",
    "    # removing background pixels (i.e. pixels with flux == 0)\n",
    "    # NOTE: noise pixels are still in the sample!\n",
    "    \n",
    "    if(PATH_cluster_file == \"data/clusters_2_smooth.fits\"):\n",
    "    # NOTE: For the smoothed image, we will apply a background thershold or else\n",
    "    #       too many data points will be used and the clustering algorithms will\n",
    "    #       crash due to memory limitations.\n",
    "        X_image = X_image[X_image[:,2]>100]\n",
    "    \n",
    "    # After dealing with background, now going back to 2D (x, y):\n",
    "    X_image = X_image[:,0:2]\n",
    "    # NOTE: We will only be using the (x,y) position of the stars (not the pixel \"intensity\"\n",
    "    \n",
    "    print(\"Image \" + str(PATH_cluster_file) + \" shape | \" + str(X_image.shape))\n",
    "        \n",
    "    datasets.append(X_image)\n",
    "    \n",
    "# > Displaying new image format:\n",
    "fig = plt.figure(figsize=(20, 4))\n",
    "fig.subplots_adjust(bottom=0.15, top=0.95, hspace=0.0, left=0.1, right=0.95, wspace=0.4)\n",
    "\n",
    "for i in range(len(PATH_cluster_files)):\n",
    "\n",
    "    X_image = datasets[i]\n",
    "    \n",
    "    ax = plt.subplot(1, 4, int(i+1))  \n",
    "    ax.scatter(X_image[:,0],X_image[:,1], c=['white','black'], s=1)\n",
    "    \n",
    "print('\\nNOTE: These are not images of stars: they only represent the positions of the stars.')\n",
    "print('Therefore, we will only be using the (x,y) position of the stars (not the pixel \"intensity\").')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RESPONSE TO B.1: Guessing the number of stellar clusters in the provided images using cluster algorithms\n",
    "HINT: To speed up the procedure, you might want to comment out the algorithms which use a fixed number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from sklearn import cluster, mixture\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import cycle, islice\n",
    "\n",
    "plt.figure(figsize=(9 * 2 + 3, 16))\n",
    "plt.subplots_adjust(left=.02, right=.98, bottom=.001, top=.96, wspace=.05,\n",
    "                    hspace=.01)\n",
    "\n",
    "plot_num = 1\n",
    "\n",
    "default_base = {'quantile': .03,\n",
    "                'eps': .05,\n",
    "                'damping': .9,\n",
    "                'preference': -3,\n",
    "                'n_neighbors': 5,\n",
    "                'n_clusters': 20,\n",
    "                'threshold':0.1}\n",
    "\n",
    "for i_dataset, dataset in enumerate(datasets):\n",
    "    params = default_base.copy()\n",
    "\n",
    "    X = dataset\n",
    "\n",
    "    # normalize dataset for easier parameter selection\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    # estimate bandwidth for mean shift\n",
    "    bandwidth = cluster.estimate_bandwidth(X, quantile=params['quantile'])\n",
    "\n",
    "    # connectivity matrix for structured Ward\n",
    "    connectivity = kneighbors_graph(\n",
    "        X, n_neighbors=params['n_neighbors'], include_self=False)\n",
    "    # make connectivity symmetric\n",
    "    connectivity = 0.5 * (connectivity + connectivity.T)\n",
    "\n",
    "    # ============\n",
    "    # Create cluster objects\n",
    "    # ============\n",
    "    ms = cluster.MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
    "    two_means = cluster.MiniBatchKMeans(n_clusters=params['n_clusters'])\n",
    "    ward = cluster.AgglomerativeClustering(\n",
    "        n_clusters=None, linkage='ward',\n",
    "        connectivity=connectivity)\n",
    "    spectral = cluster.SpectralClustering(\n",
    "        n_clusters=params['n_clusters'], eigen_solver='arpack',\n",
    "        affinity=\"nearest_neighbors\")\n",
    "    dbscan = cluster.DBSCAN(eps=params['eps'])\n",
    "    affinity_propagation = cluster.AffinityPropagation(\n",
    "        damping=params['damping'], preference=params['preference'])\n",
    "    average_linkage = cluster.AgglomerativeClustering(\n",
    "        linkage=\"average\", affinity=\"cityblock\",\n",
    "        n_clusters=params['n_clusters'], connectivity=connectivity)\n",
    "    birch = cluster.Birch(\n",
    "        n_clusters=params['n_clusters'],threshold=params['threshold'])\n",
    "    gmm = mixture.GaussianMixture(\n",
    "        n_components=params['n_clusters'], covariance_type='full')\n",
    "\n",
    "    clustering_algorithms = (\n",
    "        #('MiniBatchKMeans', two_means),\n",
    "        ('AffinityPropagation', affinity_propagation),\n",
    "        ('MeanShift', ms),\n",
    "        #('SpectralClustering', spectral),\n",
    "        #('Ward', ward),\n",
    "        #('AgglomerativeClustering', average_linkage),\n",
    "        ('DBSCAN', dbscan),\n",
    "        ('Birch', birch)\n",
    "        #('GaussianMixture', gmm)\n",
    "    )\n",
    "    # NOTE: Not considering algorithms which need the number of clusters as input\n",
    "\n",
    "    print(\"> %-30s\" % PATH_cluster_files[i_dataset])\n",
    "\n",
    "    for name, algorithm in clustering_algorithms:\n",
    "        t0 = time.time()\n",
    "\n",
    "        # catch warnings related to kneighbors_graph\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\n",
    "                \"ignore\",\n",
    "                message=\"the number of connected components of the \" +\n",
    "                \"connectivity matrix is [0-9]{1,2}\" +\n",
    "                \" > 1. Completing it to avoid stopping the tree early.\",\n",
    "                category=UserWarning)\n",
    "            warnings.filterwarnings(\n",
    "                \"ignore\",\n",
    "                message=\"Graph is not fully connected, spectral embedding\" +\n",
    "                \" may not work as expected.\",\n",
    "                category=UserWarning)\n",
    "            algorithm.fit(X)\n",
    "\n",
    "        t1 = time.time()\n",
    "        if hasattr(algorithm, 'labels_'):\n",
    "            y_pred = algorithm.labels_.astype(np.int)\n",
    "        else:\n",
    "            y_pred = algorithm.predict(X)\n",
    "\n",
    "        plt.subplot(len(datasets), len(clustering_algorithms), plot_num)\n",
    "        if i_dataset == 0:\n",
    "            plt.title(name, size=18)\n",
    "\n",
    "        colors = np.array(list(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a',\n",
    "                                             '#f781bf', '#a65628', '#984ea3',\n",
    "                                             '#999999', '#e41a1c', '#dede00']),\n",
    "                                      int(max(y_pred) + 1))))\n",
    "        plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[y_pred])\n",
    "        plt.scatter(X[y_pred==-1,0], X[y_pred==-1,1], s=100, facecolors='none', edgecolors='black', color='black', label='background', alpha=0.1, zorder=0)\n",
    "        # marking background class for DBSCAN\n",
    "\n",
    "        plt.xlim(-2.5, 2.5)\n",
    "        plt.ylim(-2.5, 2.5)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "        plt.text(.99, .01, ('%.2fs' % (t1 - t0)).lstrip('0'),\n",
    "                 transform=plt.gca().transAxes, size=15,\n",
    "                 horizontalalignment='right')\n",
    "        plot_num += 1\n",
    "\n",
    "        \n",
    "        n_clusters_ = len(set(y_pred)) - (1 if -1 in y_pred else 0)\n",
    "        n_noise_    = list(y_pred).count(-1)\n",
    "\n",
    "        print(\"  %-25s | n_clusters %-3s | n_noise points %-3s\" % (name, n_clusters_, n_noise_))\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual number of clusters\n",
    "\n",
    "| File                   | n$_{clusters}$   |\n",
    "| :--------------------: | :-: |\n",
    "| clusters_0.fits        | 22  |\n",
    "| clusters_1.fits        | 31  |\n",
    "| clusters_2.fits        | 45  |\n",
    "| clusters_2_smooth.fits | 45  |\n",
    "\n",
    "### Q: Which algorithm performs best?\n",
    "[+] click to discover\n",
    "\n",
    "[//]: # \"\n",
    "DBSCAN seems the most efficient, especially for the non-smoothed images. This is because it is able to deal with the noise (not by chance, DBSCAN is the acronym for \"Density-Based Spatial Clustering of Applications _with Noise_\").\n",
    "\n",
    "Additional post-clustering processing (e.g. rejecting clusters with a small number of samples), can further improve the results.\n",
    "\n",
    "The detection is more problematic when pixel get smoothed (as in real images). Check Bitsakis, Bonfini et al. (2017) for a technique to deal with detection in observational data.\n",
    "\"\n",
    "\n",
    "### Q: Should  we be _also_ using the 3rd dimension, i.e. the pixel intensity?\n",
    "[+] click to discover\n",
    "\n",
    "[//]: # \"\n",
    "In this case no. Because all pixels have the same value (999), therefore the pixels will appear all \"clustered\"  in the 3D dimension. This would make harder the separation of clusters.\n",
    "\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. EXERCISE: Separate stellar types \n",
    "\n",
    "The equivalent widths (EQWs) of the HeII 4200 line and HeI 4471 line can be used to separate stellar spectral types. The first one is a good indicator of **O-type** stars while the second is stronger in **early B-type** stars. The absence of both characterizes late **B-type stars**\n",
    "For more details see [Maravelias (2014), PhD thesis](http://skinakas.physics.uoc.gr/en/research/theses/GrMaraveliasPhD.pdf).\n",
    "\n",
    "### The sample\n",
    " \n",
    "From [Evans et al. (2004)](http://adsabs.harvard.edu/abs/2004MNRAS.353..601E) we selected a sample of OB stars (697) having HeII 4200 and HeI 4471 line measurements.\n",
    "\n",
    "### TASK C.1: Use K-means (or MiniBatchKMeans) to separate the 3 stellar classes\n",
    "\n",
    "### TASK C.2: Plot results and highlight cluster centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and visualizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "def flospecConv(arg):\n",
    "\t\"\"\"\n",
    "\tFunction to convert from spectral types to \n",
    "\tfloat numbers (e.g. B0,O9.5 to 20.0,19.5)\n",
    "\tand backwards.\n",
    "\t\"\"\" \n",
    "\ttry:\n",
    "\t\tfloat(arg)\n",
    "\t\tif str(arg)[0]=='1':\n",
    "\t\t\tsp = 'O'\n",
    "\t\telif str(arg)[0]=='2':\n",
    "\t\t\tsp = 'B'\n",
    "\t\telif str(arg)[0]=='3':\n",
    "\t\t\tsp = 'A'\n",
    "\t\telse:\n",
    "\t\t\tsys.exit(' ! ERROR: more than O/B stars! Adjust conversion function.')\n",
    "\t\tnew_arg = sp+str(arg)[1:]\n",
    "\texcept ValueError:\n",
    "\t\tif arg[0]=='O' or arg[0]=='o':\n",
    "\t\t\tfl = '1'\n",
    "\t\telif arg[0]=='B' or arg[0]=='b':\n",
    "\t\t\tfl = '2'\n",
    "\t\telif arg[0]=='A' or arg[0]=='a':\n",
    "\t\t\tfl = '3'\n",
    "\t\telse:\n",
    "\t\t\tsys.exit(' ! ERROR: Check input! If more than O/B stars adjust conversion function.')\n",
    "\t\tnew_arg = float(arg.replace(arg[0],fl))\n",
    "\n",
    "\treturn new_arg\n",
    "\n",
    "# Reading the data file:\n",
    "\n",
    "PATH_data = \"data/stellar_types.dat\"\n",
    "\n",
    "stars=defaultdict(list)\n",
    "with open(PATH_data,'r') as inf:\n",
    "    for line in inf:\n",
    "        if line[0]!='#':\n",
    "            cols = line.split()\n",
    "            item = cols[0]\n",
    "            spline = cols[1]\n",
    "            if spline=='HeI/4471':\n",
    "                ewHeI = cols[2]\n",
    "                stars[item].append(ewHeI)\n",
    "            elif spline=='HeII/4200':\n",
    "                ewHeII = cols[2]\n",
    "                stars[item].append(ewHeII)\n",
    "\n",
    "# Creating data structures:\n",
    "\n",
    "sptype, flosptype, ewHeI, ewHeII = [], [], [], []\n",
    "for s in stars.keys():\n",
    "    sptype.append(s.split('-')[0])\n",
    "    flosptype.append(flospecConv(s.split('-')[0]))\n",
    "    ewHeII.append(float(stars[s][0]))\n",
    "    ewHeI.append(float(stars[s][1]))\n",
    "    \n",
    "# > Organizing data in an analysis-ready fashion:\n",
    "X = np.column_stack((ewHeII,ewHeI))\n",
    "\n",
    "print('Sample shape:')\n",
    "print(\"___________________________________\")\n",
    "print('  X  | ' + str(X.shape))\n",
    "print('     | ' + str(X.shape[0]) + ' sammples x ' + str(X.shape[1]) + ' diagnostics' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOTTING\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "scat = plt.scatter(ewHeII, ewHeI, c=flosptype, edgecolors='face', cmap=\"jet\")\n",
    "cb = plt.colorbar(scat, ticks=[15,22,29])   # range of available spectral types\n",
    "cb.set_ticklabels(['O5','B2','B9'])\n",
    "cb.set_label('Spectral Types')\n",
    "\n",
    "plt.ylabel(r\"EQW of HeI $\\lambda$4471 ($\\AA$)\")\n",
    "plt.xlabel(r\"EQW of HeII $\\lambda$4200 ($\\AA$)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RESPONSE TO C.1 and C.2: Running clustering algorithm and plotting\n",
    "HINT: Check **sklearn** documentation for [KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) to know how to easily retrieve the cluster centers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Clusters_kmeans = 3\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=Clusters_kmeans, random_state=0).fit(X)\n",
    "\n",
    "print(\"Cluster centers:\")\n",
    "print(kmeans.cluster_centers_)\n",
    "\n",
    "cc_x = kmeans.cluster_centers_[:,0]\n",
    "cc_y = kmeans.cluster_centers_[:,1]\n",
    "\n",
    "plt.plot(cc_x, cc_y, 'k+', ms=100)\n",
    "\n",
    "new_map = matplotlib.cm.gray.from_list('whatever', ('blue', 'red'), N=Clusters_kmeans)\n",
    "scat2 = plt.scatter(ewHeII, ewHeI, c=kmeans.labels_, edgecolors='face', cmap=new_map)\n",
    "cb = plt.colorbar(scat2, ticks=range(0,Clusters_kmeans+1,1))   # number of clusters\n",
    "cb.set_ticklabels(range(1,Clusters_kmeans+2,1))\n",
    "cb.set_label('Cluster Label')\n",
    "\n",
    "plt.ylabel(r\"EQW of HeI $\\lambda$4471 ($\\AA$)\")\n",
    "plt.xlabel(r\"EQW of HeII $\\lambda$4200 ($\\AA$)\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
