{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Regression - Session / Workshop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "In the previous sessions have seen techniques aimed at determining the underlying P.D.F. from which the observed data are sampled (using either parametric or nonparametric models). **Regression** aims instead at inferring the expectation value of the dependent variable $y$ given the independent variable $x$ (i.e., the conditional expectation value).\n",
    "\n",
    "In the most generic form, we want to parametrize the function $f$ defined as:\n",
    "\n",
    "$$y = f (x | θ) ~~~~~~ (1)$$\n",
    "\n",
    "where **θ** is a set (array) of $k$ parameters: $θ_p$ , p = 1, . . . , $k$.\n",
    "\n",
    "For example when we think of fitting a straight line we are assuming:\n",
    "\n",
    "$$y_i = θ_0 + θ_1 x_i ~~~~~~ (2)$$\n",
    "\n",
    "Just to avoid confusion, let's clarify that we are talking about $linear$ regression when the function $f$ is linear with respect to the parameters, not with respect to the variable $x$! For example, the function:\n",
    "\n",
    "$$f (x|θ ) = \\sum^k_{p=1} θ_p g_p(x) = θ_1 g_1(x) + θ_2 g_2(x)~+~...~+~θ_k g_k(x) ~~~~~~ (3) $$\n",
    "\n",
    "describes a linear problem as long as the sub-functions $g_p(x)$ do not depend on any of the parameters $θ_p$. \n",
    "\n",
    "NOTE: The function described in Equation 3 does $not$ represent the most generic formulation of linear regression: it is just the form we will adopt in the reminder. An other example of suitable linear regression function is:\n",
    "\n",
    "$$f (x|θ ) = θ_1 g^2(x) + θ_2 g(x)h(x) + θ_2 h^2(x) ~~~~~~ (4) $$\n",
    "\n",
    "i.e. the important property is to be linear in $θ_p$.\n",
    "\n",
    "\n",
    "### 1.1. Why machine learning regression\n",
    "\n",
    "Ideally, we would like to use a theoretically-derived funtion which could describe the observed distribution of the data. By fitting its parameters to the data we could simply preform the regression (we have seen this e.g. in the Bayesian session). However this is not always possible because e.g. we might be facing a new problem or because we are simply not satisfied by the available options in the literature. \n",
    "\n",
    "Even if we do not know the intrinsic form, we can still mimic any function by using a model $f (x|θ )$ composed of an arbitrary number of sub-functions, just like in Equation 3. One common approach is to use **Basis Functions**, i.e. expand $f(x)$ over a _specific_ \"family\" of functions $g_p(x)$, for example a series of Gaussians with different means and variances.\n",
    "\n",
    "How many functions do we need? We do cannot decide that a priori. The more functions we use, the better the model will fit the data, but the number of parameters increases (i.e. the size of $θ_p$) and so does the risk overfitting. \n",
    "\n",
    "That is why we need some techniques to dinamically constrain the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ridge and LASSO techniques for likelihood penalization/regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. When parameters go crazy: the need for penalization\n",
    "\n",
    "In previous classes we saw that the best-linear regression parameters set **θ** can be found by minimizing the $\\chi^2$. In machine learning jargon, $\\chi^2$ is a type of cost function (negative likelihood is one of the possible **cost functions**). When the error bars are the same size (homoscedastic data), then the cost function can be expressed as the $L2$ norm: \n",
    "\n",
    "$$min~||~y - f (x | θ)~||^2$$\n",
    "\n",
    "However, we also saw that the risk of over-fitting is always lurking behind the corner when the fit parameters are way too many.\n",
    "In the case of the the expansion of $f$ as $\\sum^k_{p=1} θ_p g_p(x)$, too many parameters translates into the idea that we are trying to use too many base functions $g_p(x)$, so that several of them might be redundant or cancel out.\n",
    "\n",
    "One classic example is the attempt of fitting a histogram using a collection of offset Gaussians: how many are really needed?\n",
    "\n",
    "<table><tr>\n",
    "    <td width=400>\n",
    "        <img src=\"images/Bitsakis_2017_Figure_10.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 1. Color histogram for a stellar cluster in the Large Magellanic Cloud.\n",
    "            Theoptimal number of Gaussians for the fit to the data was decided via the\n",
    "            Gaussian Mixture Models technique.\n",
    "            <br>\n",
    "            (From [Bitsakis, Bonfini et al. (2017) ApJ, 845, 56]( https://ui.adsabs.harvard.edu/abs/2017ApJ...845...56B/abstract))\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "In the case of Figure 1 the problem was solved with **Gaussian Mixture Models**, which directly provides the optimal number of required Gaussians before overfitting. In other cases one can impose limits based on the **physics of the system**.\n",
    "For a generic linaer regression of the form we are discussing here, one can simply limit the number/weight of the parameters hence reducing the number of actual $g_p(x)$ contributing to the final model by **killing** some $θ_p$ (i.e. setting them to 0 for some $p$).\n",
    "\n",
    "One way to do this is by penalizing the cost function with a term $A(θ)$ which kills some values of the parameter array:\n",
    "\n",
    "$$min~||~y - f (x | θ)~||^2 + A(θ)$$\n",
    "\n",
    "\n",
    "A(θ) is usually a very simple function of θ: two famous modifications of the standard, unconstrained regression are the **Ridge** and **LASSO** regressions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Method definitions\n",
    "\n",
    "**Ridge** regression is obtained by penalizing (mind you: not completely excluding, just penalizing!) the parameter sets (**θ**) for which:\n",
    "\n",
    "$$ ||θ||^2 < s$$\n",
    "\n",
    "so that the cost function to be minimized becomes:\n",
    "\n",
    "$$min~||~y - f (x | θ)~||^2 +\\lambda~||θ||^2$$\n",
    "\n",
    "**LASSO** (**L**east **A**bsolute **S**hrinkage and **S**election + **O**) regression is obtained by penalizing (mind you: not completely excluding, just penalizing!) the parameter sets (**θ**) for which:\n",
    "\n",
    "$$ |θ| < s$$\n",
    "\n",
    "so that the cost function to be minimized becomes:\n",
    "\n",
    "$$min~||~y - f (x | θ)~||^2 +\\lambda~|θ|$$\n",
    "\n",
    "\n",
    "Where, in both cases, $s$ is an arbitrary value (not actually implemented in the fitting, but rather used to express the mathematical concept), and $\\lambda$ is an arbitrary hyper-parameter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Graphical definitions\n",
    "\n",
    "The mathematical definitions gave above correspond to the following picture on the cost function space (for a set of parameters only composed of 2 elements: **θ** = $[θ_1,θ_2]$):\n",
    "\n",
    "<table><tr>\n",
    "    <td width=400>\n",
    "        <img src=\"images/Ridge_LASSO.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 2. Restriction of the parameter space via the Ridge ($left$ \n",
    "            and LASSO ($right$) techniques.\n",
    "            <br>\n",
    "            From _\"Statistics, Data Mining, and Machine Learning in Astronomy\" - $\\S$8.2_\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "Here the concentric ellipses represent, say, the 1, 2 an 3 $\\sigma$ countours around the best-fit **θ** $if$ no penalization is applied. When we appy the penalizations, we are forcing the fit to search the best-fit parameter couple within the circle/square (NOTE: the size of the circle/square is tuned by $\\lambda$).\n",
    "\n",
    "It might look crazy that we are actually **looking away** from the best-fit solutions. However there is a very important thing to keep in mind: we are working on linear regressions in which the function $f$ is expanded over a series of arbitrary $g_p(x)$ whose weight is $θ_p$. And when we say \"**arbitrary**\" we really mean a series of functions completely detached from a physical meaning: what we want to obtain in this context is a function  $f$ which can predict our data, nothing more.\n",
    "\n",
    "From this point of view, there's nothing wrong in looking for the **simplest possible** set **θ**: we will see in fact that the **Ridge** and **LASSO** regressions get us rid of useless $g_p(x)$ components, and reduce the variance in $θ_p$.\n",
    "\n",
    "The next example will make these advantages clear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Example: fitting the distance modulus of Super-Novae (SNs) as a function of redshift\n",
    "\n",
    "_The following example is adopted from \"Statistics, Data Mining, and Machine Learning in Astronomy\" -  §§ 8_\n",
    "\n",
    "Let's suppose to have data for the distance modulus $\\mu$ of SNs as a function of $z$. This is a complicated function which we will pretend to ignore. We will instead try to expand the function $\\mu(z)$ (the predictive function we aim to derive) over a base of 100 Gaussians.\n",
    "\n",
    "For semplicity, we will assume the Gaussians all have the same $\\sigma$ = 2. The centers of the Gaussians are also fixed (one centered every $\\Delta z \\sim$ 0.025). What is left to be fit are then just the normalizations of the Gaussians. These normalizations will constitute the values of the set **θ** for this example. For each Gaussian centered at a given $z$ we will then have one $θ_p$($z$).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This block needs to be run twice\n",
    "#\n",
    "# NOTE: The code in this block is publicly available from:\n",
    "#         http://www.astroml.org/book_figures/chapter8/fig_rbf_ridge_mu_z.html#book-fig-chapter8-fig-rbf-ridge-mu-z\n",
    "#\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import lognorm\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "\n",
    "from astroML.cosmology import Cosmology\n",
    "from astroML.datasets import generate_mu_z\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# This function adjusts matplotlib settings for a uniform feel in the textbook.\n",
    "# Note that with usetex=True, fonts are rendered with LaTeX.  This may\n",
    "# result in an error if LaTeX is not installed on your system.  In that case,\n",
    "# you can set usetex to False.\n",
    "from astroML.plotting import setup_text_plots\n",
    "setup_text_plots(fontsize=8, usetex=True)\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# generate data\n",
    "np.random.seed(0)\n",
    "\n",
    "z_sample, mu_sample, dmu = generate_mu_z(100, random_state=0)\n",
    "cosmo = Cosmology()\n",
    "\n",
    "z = np.linspace(0.01, 2, 1000)\n",
    "mu = (map(cosmo.mu, z))\n",
    "# NOTE: removed \"asarray\"\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Manually convert data to a gaussian basis\n",
    "#  note that we're ignoring errors here, for the sake of example.\n",
    "def gaussian_basis(x, mu, sigma):\n",
    "    return np.exp(-0.5 * ((x - mu) / sigma) ** 2)\n",
    "\n",
    "centers = np.linspace(0, 1.8, 100)\n",
    "widths = 0.2\n",
    "X = gaussian_basis(z_sample[:, np.newaxis], centers, widths)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Set up the figure to plot the results\n",
    "fig = plt.figure(figsize=(20, 7.5))\n",
    "fig.subplots_adjust(left=0.1, right=0.95,\n",
    "                    bottom=0.1, top=0.95,\n",
    "                    hspace=0.15, wspace=0.2)\n",
    "\n",
    "classifier = [LinearRegression, Ridge, Lasso]\n",
    "kwargs = [dict(), dict(alpha=0.005), dict(alpha=0.001)]\n",
    "labels = ['Linear Regression', 'Ridge Regression', 'Lasso Regression']\n",
    "\n",
    "for i in range(3):\n",
    "    clf = classifier[i](fit_intercept=True, **kwargs[i])\n",
    "    clf.fit(X, mu_sample)\n",
    "    w = clf.coef_\n",
    "    fit = clf.predict(gaussian_basis(z[:, None], centers, widths))\n",
    "    \n",
    "    # plot fit\n",
    "    ax = fig.add_subplot(231 + i)\n",
    "    ax.xaxis.label.set_size(20)\n",
    "    ax.yaxis.label.set_size(20)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "    ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "\n",
    "    # plot curves for regularized fits\n",
    "    if i == 0:\n",
    "        ax.set_ylabel('$\\mu$')\n",
    "    else:\n",
    "        ax.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "        curves = 37 + w * gaussian_basis(z[:, np.newaxis], centers, widths)\n",
    "        curves = curves[:, abs(w) > 0.01]\n",
    "        ax.plot(z, curves,\n",
    "                c='gray', lw=1, alpha=0.5)\n",
    "       \n",
    "    mu = list(mu)\n",
    "    # NOTE: converting map to list or else plotting won't work\n",
    "    \n",
    "    ax.plot(z, fit, '-k')\n",
    "    ax.plot(z, mu, '--', c='gray')\n",
    "\n",
    "    ax.errorbar(z_sample, mu_sample, dmu, fmt='.k', ecolor='gray', lw=1, ms=4)\n",
    "    ax.set_xlim(0.001, 1.8)\n",
    "    ax.set_ylim(36, 52)\n",
    "    ax.text(0.05, 0.93, labels[i],\n",
    "            size=20,\n",
    "            ha='left', va='top',\n",
    "            bbox=dict(boxstyle='round', ec='k', fc='w'),\n",
    "            transform=ax.transAxes)\n",
    "\n",
    "    # plot weights\n",
    "    ax = plt.subplot(234 + i)\n",
    "    ax.xaxis.label.set_size(20)\n",
    "    ax.yaxis.label.set_size(20)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "    ax.xaxis.set_major_locator(plt.MultipleLocator(0.5))\n",
    "    ax.set_xlabel('$z$')\n",
    "    if i == 0:\n",
    "        ax.set_ylabel(r'$\\theta$')\n",
    "        w *= 1E-12\n",
    "        ax.text(0, 1.01, r'$\\rm \\times 10^{12}$',\n",
    "                transform=ax.transAxes)\n",
    "    ax.scatter(centers, w, s=9, lw=0, c='k')\n",
    "\n",
    "    ax.set_xlim(-0.05, 1.8)\n",
    "\n",
    "    if i == 1:\n",
    "        ax.set_ylim(-2, 4)\n",
    "    elif i == 2:\n",
    "        ax.set_ylim(-0.5, 2)\n",
    "\n",
    "    ax.text(0.05, 0.93, labels[i],\n",
    "            size=20,\n",
    "            ha='left', va='top',\n",
    "            bbox=dict(boxstyle='round', ec='k', fc='w'),\n",
    "            transform=ax.transAxes)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: What do you observe?\n",
    "[+] click to discover\n",
    "\n",
    "[//]: # \"\n",
    "* Unconstrained linear regression drastically overfits. Moreover the range of parameters (and their distribution's variance) is very large. Looks like one component cancels out with the next one, therefore a lot of components are needed.\n",
    "\n",
    "* Ridge regression \"regularizes\" the distribution of the parameter values, but introduces a sinusoidal correlation (?).\n",
    "\n",
    "* LASSO regression crops out several Gaussian components. Why does this happen? Think of the graphical definition shown above. Naturally the algorithm tries to reproduce the unconstrained set **θ**. However due to the limitation we impose, it cannot grasp both the unconstrained values. As a result, the best-fit parameters are seeked for in the \"corners\" of the limiting square, where at least one of the parameter is optimized. However, in a corner, while one parameter is maximized the next is set to almost 0.\n",
    "\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Final remarks on Ridge/LASSO penalization\n",
    "\n",
    "### Pros\n",
    "* Good when data points are few but still want to expand the function $f$ over a family of functions $g_p(x)$ while limiting over-fitting\n",
    " \n",
    "### Cons\n",
    "* The selection of the surviving \"significant parameters\" $θ_p$ is extremely arbitrary: not good for defining physically meaningful functions (e.g. do not try to fit a series of Schechter functions to a galaxy population to define its demographics)\n",
    "* Introduces parameter correlation\n",
    "\n",
    "### Caveat\n",
    "As usual, the value $\\lambda$ must be optimized using a cross-validation sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Nadaraya - Watson Kernel Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's step back and try to construct a very simple regression model to fit our data: the average over a vertical slice. \n",
    "\n",
    "Consider the red vertical line at $\\bar x=4$ in Figure 3.1. We can compute the mean $\\langle$$\\bar y$$\\rangle$ of the $y$ values corresponding to $\\bar x$. If we repeat this operation for every $x$ we can obtain a regression function (showed as a red curve in Figure 3.1):\n",
    "\n",
    "$$f(x)=\\frac{\\sum_{n=1}^{N}y_{i}}{N}$$\n",
    "\n",
    "<table><tr>\n",
    "    <td width=400>\n",
    "        <img src=\"images/regression_mean.jpg\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 3.1. Regression performed using the average along a vertical line.\n",
    "            <br>\n",
    "            [From here](https://gerardnico.com/data_mining/knn)\n",
    "        </center>\n",
    "    </td>\n",
    "    <td width=400>\n",
    "        <img src=\"images/regression_neighborhood.jpg\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 3.2. Regression performed using the average along a vertical slice.\n",
    "            <br>\n",
    "            [From here](https://gerardnico.com/data_mining/knn)\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "In reality though, there are never enough data to compute a  $\\langle$$y$$\\rangle$ value for every $x$ (see Figure 3.2: this time, at $\\bar x$ there is no $y$ data point). We can however compute the mean  $\\langle$$\\bar y$$\\rangle$ within a small interval around $\\bar x$ (vertical dotted lines). \n",
    "\n",
    "For a given interval, data points which are closer to $\\bar x$ should affect the $\\bar{y}$ computation more than the further ones. To account for this we can introduce a weight term ($w_{i}$) in the calculation of the mean (we can think of this as a **locally weighted mean regression**):\n",
    "\n",
    "$$f(x)=\\frac{1}{N} {\\sum_{n=1}^{N}w_{i}y_{i}}$$\n",
    "\n",
    "The $w_{i}$ term can be implemented through the definition a Kernel function which weights by the distance $|x_{i}- \\bar x|$ from the point $\\bar x$:\n",
    "\n",
    "$$K=K( \\frac{|x_{i}- \\bar x|}{h} )$$\n",
    "\n",
    "where the term $h$ (**bandwidth**) regulates the influence of the kernel and it is usually more critical than the form of the Kernel itself, because it can cause over/under-fitting. Its estimation can be achieved by using Jacknife cross-valdidation techniques which are sometimes automatically imprelemented in regression algorithms.\n",
    "\n",
    "The weight term is then simply the normalized kernel:\n",
    "\n",
    "$$w_{i}=\\frac{K \\Large(\\frac{|x_{i}-x|}{h} \\Large)}{\\sum_{n=1}^{N} K \\Large(\\frac{|x_{i}-x|}{h} \\Large)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Example: fitting the distance modulus of Super-Novae (SNs) as a function of redshift\n",
    "\n",
    "_The following example is adopted from \"Statistics, Data Mining, and Machine Learning in Astronomy\" -  §§ 8_\n",
    "\n",
    "Let's apply the Kernel Regression to the SN example presented in $\\S$ 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: The code in this block is publicly available from:\n",
    "#         http://www.astroml.org/book_figures/chapter8/fig_regression_mu_z.html\n",
    "#\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import lognorm\n",
    "\n",
    "from astroML.cosmology import Cosmology\n",
    "from astroML.datasets import generate_mu_z\n",
    "from astroML.linear_model import LinearRegression, PolynomialRegression,\\\n",
    "    BasisFunctionRegression, NadarayaWatson\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# This function adjusts matplotlib settings for a uniform feel in the textbook.\n",
    "# Note that with usetex=True, fonts are rendered with LaTeX.  This may\n",
    "# result in an error if LaTeX is not installed on your system.  In that case,\n",
    "# you can set usetex to False.\n",
    "from astroML.plotting import setup_text_plots\n",
    "setup_text_plots(fontsize=8, usetex=True)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Generate data\n",
    "z_sample, mu_sample, dmu = generate_mu_z(100, random_state=0)\n",
    "\n",
    "cosmo = Cosmology()\n",
    "z = np.linspace(0.01, 2, 1000)\n",
    "mu_true = (map(cosmo.mu, z))\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Define our classifiers\n",
    "basis_mu = np.linspace(0, 2, 15)[:, None]\n",
    "basis_sigma = 3 * (basis_mu[1] - basis_mu[0])\n",
    "\n",
    "subplots = [221, 222, 223, 224]\n",
    "classifiers = [LinearRegression(),\n",
    "               PolynomialRegression(4),\n",
    "               BasisFunctionRegression('gaussian',\n",
    "                                       mu=basis_mu, sigma=basis_sigma),\n",
    "               NadarayaWatson('gaussian', h=0.1)]\n",
    "text = ['Straight-line Regression',\n",
    "        '4th degree Polynomial\\n Regression',\n",
    "        'Gaussian Basis Function\\n Regression',\n",
    "        'Gaussian Kernel\\n Regression']\n",
    "\n",
    "# number of constraints of the model.  Because\n",
    "# Nadaraya-watson is just a weighted mean, it has only one constraint\n",
    "n_constraints = [2, 5, len(basis_mu) + 1, 1]\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "fig.subplots_adjust(left=0.1, right=0.95,\n",
    "                    bottom=0.1, top=0.95,\n",
    "                    hspace=0.05, wspace=0.05)\n",
    "\n",
    "for i in range(4):\n",
    "    ax = fig.add_subplot(subplots[i])\n",
    "\n",
    "    # fit the data\n",
    "    clf = classifiers[i]\n",
    "    clf.fit(z_sample[:, None], mu_sample, dmu)\n",
    "\n",
    "    mu_sample_fit = clf.predict(z_sample[:, None])\n",
    "    mu_fit = clf.predict(z[:, None])\n",
    "\n",
    "    chi2_dof = (np.sum(((mu_sample_fit - mu_sample) / dmu) ** 2)\n",
    "                / (len(mu_sample) - n_constraints[i]))\n",
    "\n",
    "    mu_true = list(mu_true)\n",
    "    # NOTE: converting map to list or else plotting won't work\n",
    "    \n",
    "    ax.plot(z, mu_fit, '-k')\n",
    "    ax.plot(z, mu_true, '--', c='gray')\n",
    "    ax.errorbar(z_sample, mu_sample, dmu, fmt='.k', ecolor='gray', lw=1)\n",
    "\n",
    "    ax.text(0.5, 0.05, r\"$\\chi^2_{\\rm dof} = %.2f$\" % chi2_dof, size=20,\n",
    "            ha='center', va='bottom', transform=ax.transAxes)\n",
    "\n",
    "    ax.set_xlim(0.01, 1.8)\n",
    "    ax.set_ylim(36.01, 48)\n",
    "    ax.text(0.05, 0.95, text[i], ha='left', va='top',\n",
    "            size=20,\n",
    "            transform=ax.transAxes)\n",
    "    ax.xaxis.label.set_size(20)\n",
    "    ax.yaxis.label.set_size(20)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "    \n",
    "    if i in (0, 2):\n",
    "        ax.set_ylabel(r'$\\mu$')\n",
    "    else:\n",
    "        ax.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "\n",
    "    if i in (2, 3):\n",
    "        ax.set_xlabel(r'$z$')\n",
    "    else:\n",
    "        ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: What do we observe?\n",
    "\n",
    "[+] click to discover\n",
    "\n",
    "[//]: # \"\n",
    "* Kernel Regression is less sensitive to data gaps than Gaussian Basis, at least in this example. \n",
    "\n",
    "* Kernel Regression diverges from the data for large and small $z$. Simply, the interval is ill-defined there since it is not symmetric.\n",
    "\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.2. Final remarks on Ridge/LASSO penalization\n",
    "\n",
    "### Pros\n",
    "\n",
    "* Simple and intuitive\n",
    "\n",
    "* Useful with noisy data where the underlying functional form is not clear\n",
    "\n",
    "### Cons\n",
    "\n",
    "* Computationally intensive\n",
    "\n",
    "* May not be very accurate when the sample has a few data points\n",
    "  <br>\n",
    "  _(Choosing the right bandwidth value can mitigate this problem)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. EXERCISE: Use regression to fit and subtract galaxy continuum\n",
    "\n",
    "In previous sessions (e.g. Introduction and MCMC) we have explored some methods to fit spectral lines.\n",
    "In this exercise, we want to isolate the spectral emission lines of a late-type galaxy (e.g. a spiral) by removing the stellar continuum from the spectrum.\n",
    "\n",
    "This procedure represents a very common step in the spectral data reductions, and it is performed through a variety of methods. The simplest approaches focus on a line at a time, removing only the local continuum by fitting it e.g. with a polynomial. Sophisticated methods fit the whole spectrum using appropriate stellar population templates. Here we will use linear regression as a quick method to obtain a fast continuum subtraction without dealing with detailed tuning.\n",
    "\n",
    "We will use an optical spectrum from SDSS from the\n",
    "[Spectral cross-correlation templates](http://classic.sdss.org/dr5/algorithms/spectemplates/)\n",
    "\n",
    "### TASK A.1: Use the Gaussian Basis expansion and the Ridge/LASSO regularization to fit the spectrum\n",
    "\n",
    "HINT: Try to run the regression with the full spectrum, and then removing the features we are most interested in isolating.\n",
    "\n",
    "### TASK A.2: Compare your results with the background-subtracted spectrum distributed along with the SDSS data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the SDSS spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADING THE SDSS SPECTRUM\n",
    "#\n",
    "# To understand the SDSS file format, consult:\n",
    "#   http://www.sdss2.org/dr3/dm/flatFiles/spSpec.html\n",
    "\n",
    "# FITS manipulation:\n",
    "from astropy.io import fits\n",
    "\n",
    "PATH_spectrum = \"data/late-type.fits\"\n",
    "    \n",
    "hdulist = fits.open(PATH_spectrum)\n",
    "data = hdulist[0].data\n",
    "# Header keywords to perform wavelength calibration:\n",
    "coeff0 = hdulist[0].header['coeff0']\n",
    "coeff1 = hdulist[0].header['coeff1']\n",
    "spectrum_flux  = data[0] # observed spectrum\n",
    "spectrum_noBG  = data[1] # contimuum-subtracted spectrum\n",
    "hdulist.close()\n",
    "\n",
    "spectrum_flux = spectrum_flux[:-100]\n",
    "spectrum_noBG = spectrum_noBG[:-100]\n",
    "# NOTE: Removing last 100 data points, which are bogus\n",
    "\n",
    "# Creating arbitrary uncertainty array proportional to the flux:\n",
    "spectrum_flux_err = spectrum_flux * np.random.uniform(0.01, 0.1, size=len(spectrum_flux))\n",
    "\n",
    "# Creating wavelength array using header calibrations:\n",
    "spectrum_wave = 10.0 ** (coeff0 + coeff1 * np.arange(len(spectrum_flux)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the spectrum\n",
    "\n",
    "In plotting, we will highlight 2 strong lines in the spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining ranges of strong features:\n",
    "#   http://astronomy.nmsu.edu/drewski/tableofemissionlines.html\n",
    "feature_0 = [6550, 6580] # H_alpha 6562.819\n",
    "feature_1 = [6710, 6750] # SII 6730.810\n",
    "# wavelength min and max for each feature [A]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# > Displaying spectrum:    \n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "fig.subplots_adjust(bottom=0.15, top=0.95, hspace=0.0, left=0.1, right=0.95, wspace=0.4)\n",
    "\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "ax.set_title('Late-type galaxy spectrum', fontsize=14)\n",
    "\n",
    "ax.set_xlabel('$\\lambda$ [A]', fontsize=14)\n",
    "ax.set_ylabel('$f_{v}$ [Jy]',  fontsize=14)\n",
    "#\n",
    "ax.set_yscale('log')\n",
    "\n",
    "ax.tick_params(axis = 'both', which = 'major', labelsize = 14)\n",
    "ax.tick_params(axis = 'both', which = 'minor', labelsize = 14)\n",
    "\n",
    "ax.errorbar(spectrum_wave, spectrum_flux, spectrum_flux_err, fmt='blue', ecolor='gray', lw=1, ms=4)\n",
    "\n",
    "# Marking emission features:\n",
    "ax.axvspan(feature_0[0], feature_0[1] , color='grey', alpha=0.2)\n",
    "ax.axvspan(feature_1[0], feature_1[1],  color='grey', alpha=0.2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RESPONSE TO A.1: Using the Gaussian Basis expansion and the Ridge/LASSO regularization to fit the spectrum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wave        = spectrum_wave\n",
    "flux        = spectrum_flux\n",
    "flux_err    = spectrum_flux_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL PARAMETERS:\n",
    "\n",
    "# Manually converting data to a gaussian basis:\n",
    "def gaussian_basis(x, mu, sigma):\n",
    "    return np.exp(-0.5 * ((x - mu) / sigma) ** 2)\n",
    "\n",
    "n_gaussians = ...\n",
    "centers = np.linspace(np.min(wave), np.max(wave), n_gaussians)\n",
    "widths = ...\n",
    "\n",
    "X = gaussian_basis(wave[:, np.newaxis], centers, widths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Most of the code in this block represents an edited version of:\n",
    "#         http://www.astroml.org/book_figures/chapter8/fig_rbf_ridge_mu_z.html#book-fig-chapter8-fig-rbf-ridge-mu-z\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# This function adjusts matplotlib settings for a uniform feel in the textbook.\n",
    "# Note that with usetex=True, fonts are rendered with LaTeX.  This may\n",
    "# result in an error if LaTeX is not installed on your system.  In that case,\n",
    "# you can set usetex to False.\n",
    "from astroML.plotting import setup_text_plots\n",
    "setup_text_plots(fontsize=8, usetex=True)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Set up the figure to plot the results\n",
    "\n",
    "x_plot = np.linspace(np.min(wave), np.max(wave), 1000)\n",
    "# sampled array for plotting purposes\n",
    "\n",
    "fig = plt.figure(figsize=(20, 7.5))\n",
    "fig.subplots_adjust(left=0.1, right=0.95, bottom=0.1, top=0.95, hspace=0.15, wspace=0.2)\n",
    "\n",
    "classifier = [LinearRegression, Ridge, Lasso]\n",
    "kwargs = [dict(), dict(alpha=0.00005), dict(alpha=0.00001)]\n",
    "labels = ['Linear Regression', 'Ridge Regression', 'Lasso Regression']\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "    clf = classifier[i](fit_intercept=True, **kwargs[i])\n",
    "    clf. ...\n",
    "    w = clf.coef_\n",
    "    fit = ...\n",
    "    \n",
    "    # plot fit\n",
    "    ax = fig.add_subplot(231 + i)\n",
    "    ax.xaxis.label.set_size(20)\n",
    "    ax.yaxis.label.set_size(20)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "    ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "    #\n",
    "    ax.set_yscale('log')\n",
    "\n",
    "    # plot curves for regularized fits\n",
    "    if i == 0:\n",
    "        ax.set_ylabel('$f_{v}$ [Jy]')\n",
    "    else:\n",
    "        ax.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "        axis_ymin, axis_ymax = min(flux), max(flux)\n",
    "        w_plot = (0.3 * w / max(w)) * (axis_ymax - axis_ymin) + axis_ymin\n",
    "        curves = w_plot * gaussian_basis(x_plot[:, np.newaxis], centers, widths)\n",
    "\n",
    "        ax.plot(x_plot, curves, c='gray', lw=1, alpha=0.5)\n",
    "    \n",
    "    # NOTE: converting map to list or else plotting won't work\n",
    "    \n",
    "    ax.plot(x_plot, fit, '-k')\n",
    "\n",
    "    ax.errorbar(wave, flux, flux_err, fmt='.k', ecolor='gray', lw=1, ms=4)\n",
    "    ax.set_xlim(np.min(wave),np.max(wave))\n",
    "    ax.set_ylim(np.min(flux),np.max(flux))\n",
    "    ax.text(0.05, 0.93, labels[i],\n",
    "            size=20,\n",
    "            ha='left', va='top',\n",
    "            bbox=dict(boxstyle='round', ec='k', fc='w'),\n",
    "            transform=ax.transAxes)\n",
    "    \n",
    "    \n",
    "    # plot weights\n",
    "    ax = plt.subplot(234 + i)\n",
    "    ax.xaxis.label.set_size(20)\n",
    "    ax.yaxis.label.set_size(20)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "    #ax.xaxis.set_major_locator(plt.MultipleLocator(0.5))\n",
    "    ax.set_xlabel('$\\lambda$ [A]')\n",
    "    if i == 0:\n",
    "        ax.set_ylabel(r'$\\theta$')\n",
    "        w *= 1E-12\n",
    "        ax.text(0, 1.01, r'$\\rm \\times 10^{12}$',\n",
    "                transform=ax.transAxes)\n",
    "    ax.scatter(centers, w, s=9, lw=0, c='k')\n",
    "\n",
    "    ax.set_xlim(np.min(centers), np.max(centers))\n",
    "\n",
    "    ax.set_ylim(np.min(w), np.max(w))\n",
    "\n",
    "    ax.text(0.05, 0.93, labels[i],\n",
    "            size=20,\n",
    "            ha='left', va='top',\n",
    "            bbox=dict(boxstyle='round', ec='k', fc='w'),\n",
    "            transform=ax.transAxes)\n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RESPONSE TO A.2: Comparing background subtraction with SDSS results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESAMPLING LASSO FITTED CURVE TO DATA\n",
    "#\n",
    "# In order to plot the fitting curve produced by the classifier (\"fit\")\n",
    "#   along with \"spectrum_flux\", we first resample it to the same\n",
    "#   array, i.e. \"sample_wave\"\n",
    "# This is because \"fit\" has been sampled on \"x_plot\", which has\n",
    "#   a different sampling than \"spectrum_wave\"\n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "clf = classifier[2](fit_intercept=True, **kwargs[i]) # LASSO\n",
    "clf.fit(X, flux)\n",
    "\n",
    "fit = clf.predict(gaussian_basis(x_plot[:, None], centers, widths))\n",
    "\n",
    "f = interp1d(x_plot, fit, kind='cubic')\n",
    "# interpolation function\n",
    "# - linear\n",
    "# - quadratic\n",
    "# - cubic\n",
    "fit_interp = f(spectrum_wave) \n",
    "\n",
    "\n",
    "# PLOTTING\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# > Displaying spectrum:    \n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "fig.subplots_adjust(bottom=0.15, top=0.95, hspace=0.0, left=0.1, right=0.95, wspace=0.4)\n",
    "\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "ax.set_title('Late-type galaxy - continuum-subracted spectrum', fontsize=14)\n",
    "\n",
    "ax.set_xlabel('$\\lambda$ [A]', fontsize=14)\n",
    "ax.set_ylabel('$f_{v}$ [Jy]', fontsize=14)\n",
    "\n",
    "ax.tick_params(axis = 'both', which = 'major', labelsize = 14)\n",
    "ax.tick_params(axis = 'both', which = 'minor', labelsize = 14)\n",
    "\n",
    "ax.plot ... # plot continuum-subtracted spectrum from our analysis\n",
    "ax.plot ... # plot continuum-subtracted spectrum from SDSS\n",
    "\n",
    "ax.legend(fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
